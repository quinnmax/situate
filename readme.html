<h1 id="situate">Situate</h1>

<p>Situate is a system for active grounding of situations in images. <br />
[edit: link to icsc paper]</p>

<h1 id="runningsituate">Running Situate</h1>

<h3 id="experimentparameters">Experiment parameters</h3>

<p>The <em>experiment parameters</em> file specifies:</p>

<ul>
<li>the situation definition</li>

<li>the Situate running parameters</li>

<li>the training and testing images</li>

<li>visualization settings</li>
</ul>

<p>example: "parameters<em>experiment</em>dogwalking_viz.json"</p>

<h4 id="situationdefinition">Situation definition</h4>

<p>The <em>situation definition</em> file specifies </p>

<ul>
<li>the situation objects that make up the situation</li>

<li>a mapping from possible labels (found in training data) to the situation objects</li>
</ul>

<p>example: "situation_definitions/dogwalking.json"</p>

<h4 id="situaterunningparameters">Situate running parameters</h4>

<p>The <em>Situate running parameters</em> file defines the functions that are used by situate. These includes the functions used to train and apply the classifier, to define and apply the conditional relationships among objects, and numerous others.</p>

<p>example: "parameters<em>situate</em>default.json"</p>

<p>A list of several <em>Situate running parameters</em> files can be included in the <em>experiment parameters file</em>. If this is the case, each parameterization will be run.</p>

<h4 id="trainingandtestingimagedirectories">Training and testing image directories</h4>

<pre><code>"directory_train" : "/Users/Max/Documents/images/DogWalking_PortlandSimple_train/",
"directory_test"  : "/Users/Max/Documents/images/DogWalking_PortlandSimple_test/",
</code></pre>

<h4 id="additionalsettings">Additional settings</h4>

<pre><code>"use_parallel"                  : 0, 
"run_analysis_after_completion" : 0,
"num_folds"                     : 1,
"use_visualizer"                : 0,
"specific_folds"                : [1],
"max_testing_images"            : "",
"testing_seed"                  : ""
</code></pre>

<h3 id="run">Run</h3>

<p>To run Situate, call with an experiment parameters file</p>

<pre><code>situate.experiment_run( 'parameters_experiment_dogwalking_viz.json' );
</code></pre>

<h1 id="definingasituation">Defining a Situation</h1>

<p>Defining a new situation requires a file that defines the situation and set of positive training example images that have labels specifying the relevant objects in each image. </p>

<h3 id="situationdefinitionfile">Situation definition file</h3>

<p>Situation definition files are in JSON format. They include a description of the situation and a list of constituent objects. The constituent objects have a list of labels that may be present in the training data. </p>

<pre><code>{ 
    "situation":{

        "description" : "urn",

        "situation_objects" : {

            "black_ball" : {
                "possible_labels" : [
                    "black_ball_big",
                    "black_ball_small"],
                "urgency_pre"  : "1.0",
                "urgency_post" : ".25"
            },

            "white_ball" : {
                "possible_labels" : [
                    "white_ball_big",
                    "white_ball_small"],
                "urgency_pre"  : "1.0",
                "urgency_post" : ".25"
            }
        }
    }
}
</code></pre>

<p>Situation definition files are stored in <em>situation_definitions/</em></p>

<h3 id="labeleddata">Labeled data</h3>

<p>The relationships between the constituent objects are inferred from the training data. Each training image should have a label file that specifies tight bounding boxes for situation objects in the image.</p>

<p>Labels can be generated with the tool:</p>

<pre><code>labels_generate('my_image_directory/');
</code></pre>

<p>These images and label files will be used to train:</p>

<ul>
<li>the situation model</li>

<li>visual classifiers [edit:cite]</li>

<li>bounding box regressors [edit:cite]</li>
</ul>

<p>The reliability of the individual classifiers can be estimated and used to weight the classifiers contribution to situation detections.</p>

<h1 id="specifyingtrainingandtestingsets">Specifying training and testing sets</h1>

<p>Situate can take a bit of time to train models, so it's nice to identify when existing models for a particular training set can be used. To this end, there are several ways to specify the training and testing sets to be used by Situate.</p>

<p>Specifying images for training and testing is done with the following variables in an <em>experiment parameters</em> file:</p>

<pre><code>"directory_train"                  : "",
"directory_test"                   : "",
"training_testing_split_directory" : "",
"num_folds"                        : "",
"specific_folds"                   : [],
"max_testing_images"               : "",
"testing_seed"                     : ""
</code></pre>

<p>Together, these over determine the specification of training and testing images, so below are a few things one might want to do, and how they would be specified.</p>

<h4 id="separatedirectoriesfortrainingtesting">Separate directories for training/testing</h4>

<p>To train on all images in a directory (say, folder<em>a/) and test on all images in a separate directory (folder</em>b/), the parameters should be set as below:</p>

<pre><code>"directory_train"                  : "folder_a/",
"directory_test"                   : "folder_b/",
"training_testing_split_directory" : "",
"num_folds"                        : "",
"specific_folds"                   : [],
"max_testing_images"               : "",
"testing_seed"                     : ""
</code></pre>

<h4 id="specifyingsingledirectoryfilelist">Specifying single directory, file list</h4>

<p>If all images are in a single directory and the training and testing images are specified by lists of image names in a file, the parameters should be set as below:</p>

<pre><code>"directory_train"                  : "folder_a/",
"directory_test"                   : "folder_a/",
"training_testing_split_directory" : "data_splits/example_split/",
"num_folds"                        : "",
"specific_folds"                   : [],
"max_testing_images"               : "",
"testing_seed"                     : ""
</code></pre>

<p>The folder <em>data_splits/example_split/</em> should contain at least two text files with the naming format </p>

<pre><code>[*]fnames_split_[n]_train.txt
[*]fnames_split_[n]_test.txt
</code></pre>

<p>where <em>n</em> indicates which split these files specify, and contents that include one file name per line without path information. For example:</p>

<pre><code>situate_fnames_split_1_train.txt

situation_image_001.jpg
situation_image_002.jpg
situation_image_003.jpg
</code></pre>

<p>and</p>

<pre><code>situate_fnames_split_1_test.txt

situation_image_004.jpg
situation_image_005.jpg
</code></pre>

<h4 id="specifyingseparatedirectoriesandincludingfilelists">Specifying separate directories and including file lists</h4>

<p>If you would like to use a subset of the available training and testing data present in separate directories, you can specify both separate folders and a folder contain file lists.  Parameters should be set as below:</p>

<pre><code>"directory_train"                  : "folder_a/",
"directory_test"                   : "folder_b/",
"training_testing_split_directory" : "data_splits/example_split/",
"num_folds"                        : "",
"specific_folds"                   : [],
"max_testing_images"               : "",
"testing_seed"                     : ""
</code></pre>

<h4 id="specifyingspecificfoldstorun">Specifying specific folds to run</h4>

<p>If there are files that define multiple folds, then the variable <em>specific_folds</em> can be used to specify which folds should be run. For example, if there are five folds defined that have image file lists and you want to run only the 2nd and 4th folds, then the parameters should be set as below</p>

<pre><code>"directory_train"                  : "folder_a/",
"directory_test"                   : "folder_a/",
"training_testing_split_directory" : "data_splits/example_split/",
"num_folds"                        : "",
"specific_folds"                   : [2,4],
"max_testing_images"               : "",
"testing_seed"                     : ""
</code></pre>

<h4 id="specifyingsingledirectoryseedvalue">Specifying single directory, seed value</h4>

<p>If all images are in a single directory, and you'd like to have situate generate a split between training and testing images, then you have several options. </p>

<p>You can specify the number of folds, which in turn defines the number of training images per fold. That is, if you set <em>num_folds</em> to 3, then 2/3 of images will be used for training, 1/3 for testing. Note: if <em>num_folds</em> is set to 1, then 25% of the data is used for testing. </p>

<p>parameters should be set as below:</p>

<pre><code>"directory_train"                  : "folder_a/",
"directory_test"                   : "folder_a/",
"training_testing_split_directory" : "",
"num_folds"                        : "",
"specific_folds"                   : [],
"max_testing_images"               : "",
"testing_seed"                     : ""
</code></pre>

<p>When the same folder is specified for training and testing and no split files are provided, split files will be generated and saved in 
<em>data_splits/[situation description]_[time stamp]/</em></p>

<h4 id="includingmaximumnumberoftestingimages">Including maximum number of testing images</h4>

<p>For any of the above methods, a maximum number of testing images to run on can be specified and will simply limit the number of images that Situate will run on. It will not change how many images are used for training. For example, if there are 100 images included in the testing image directory,</p>

<pre><code>"max_testing_images" : 10,
</code></pre>

<p>will cause Situate to run on only the first 10 images in that directory.</p>

<h1 id="analysis">Analysis</h1>

<p>The results of a Situate experiment are stored in the <em>results/</em> folder. There will be a folder for the experiment and individual .mat files for each of the parameterizations that were run during the experiment. </p>

<p>There are several scripts available for looking at the results of the experiment.  </p>

<h3 id="visualizingfinalworkspaces">Visualizing final workspaces</h3>

<pre><code>situate_experiment_analysis_output_final_workspaces.m 
</code></pre>

<p>This script generates images displaying the final predicted bounding boxes for situation objects overlaid on the original images. It can be helpful for a subjective analysis of the quality of results. The input should be the path to a .mat file from the experiment's results directory. The script will generate a new folder in the experiment's results directory with images for each of the final workspaces included in the .mat file. </p>

<pre><code>situate_experiment_analysis_output_final_workspaces('results/my_experiment/params1_results.mat');
</code></pre>

<h3 id="positiveinstancegroundingcomparison">Positive instance grounding comparison</h3>

<pre><code>situate_experiment_analysis.m
</code></pre>

<p>This script evaluates the results for a collection of test images. It can be used to compare multiple Situate parameterizations run on the same collection of test images. It produces several figures that relate the number of iterations run by a Situate parameterization to the number of situation detections made.  Results are also broken down by individual object types. </p>

<p>To run this script, you need to provide the path to a directory the .mat file results for the parameterizations that you would like to compare. For example</p>

<pre><code>situate_experiment_analysis('results/my_experiment/');
</code></pre>

<p>The analysis will run on and compare each of the .mat files in the directory. Figures will be saved to the provided results directory.</p>

<h3 id="imageretrievalresults">Image retrieval results</h3>

<pre><code>situate_experiment_analysis_PR.m
</code></pre>

<p>This script compares the results of several parameterizations with respect to images that contain the the situation and images that do not (whereas the previous script was only concerned with generating a grounding for positive images). Situate is run on each image, producing a final workspace, which is then used to generate a single-valued <em>situation score</em>. The two evaluation metrics produces are:</p>

<ul>
<li>Median rank: The situation scores for a single positive image and all negative images are sorted. The rank for the positive image is the number of negative images that have a higher situation score + 1. The median rank is the median of the rank over all positive images evaluated.</li>

<li>Average recall at <em>n</em>: The situation scores for a single positive image and all negative images are sorted. If the rank of the positive image is less than or equal to <em>n</em>, then it is given a recall score of 1. Otherwise, its recall score is 0. The mean recall score is taken over all positive images.</li>
</ul>

<p>To run this script, you must provide the path to a directory containing results from a run on images that do contain the situation (path<em>pos), and the path to a directory containing results from a run on images that do not contain the situation (path</em>neg). For example</p>

<pre><code>path_pos = 'results/my_experiment_pos/';
path_neg = 'results/my_experiment_neg/';
[ mean_recall_at, median_rank ] = situate_experiment_analysis_PR( path_pos, path_neg );
</code></pre>

<p>CSV files containing the results are also written to </p>

<pre><code>results/PR_results [current date and time]/
</code></pre>