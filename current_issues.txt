
current
		
	include setup script
		
		x include some built-in models that identifies the test_script_images as an acceptable training set

		x check for matconvnet and try to compile
		x download the pre-trained network
		x add folders that are expected but not included in the repo (local only, external box data, etc)
		
		x when matconvnet fails to compile, try to catch that and direct the user to the matconvnet site with compilation advice

	sundries

		check running in parallel mode

		x there's still a warning for a label file not being present on a testing image. shouldn't do that

		x if images aren't found, need to display what exactly it was looking for that wasn't found. this should be in the situate.experiment_handler code

		x situation model train calls things p struct, but is always situation struct, fix

	analysis unit test

		x single run filename, positives
		x single run filename, negatives
		x list of directories, pos, neg
		x single directory, multiple files
		x single directory, single file
		x two files not in the results directory at all

	analysis issues

		the per-object detection rates are just for positives right now - should include precision recall for each object when pos and neg images are present (although this is challenging, as dogs are present in negatives, but not labeled)
		x close figures in analysis script, with flag for keep open
		x change name of "rcnn box data" to "external box data" and update the readme
		x recall at n x axis goes too far. 100 images, shows 200

	readme

		x include link to portland simple dogwalking in the readme
			http://web.cecs.pdx.edu/~mm/PortlandStateDogWalkingImages.html
		x note: the jsons are not in there yet, just the .labl files
		x label x-axis on the distribution of support scores figure
		x test script
			tries training on a few images
			has some existing models to apply to a few images
			package up the trained validation models
			include a script that runs those

	report

		discuss what changed to make repeated instances of objects work, limitations

		discuss how situate would really be used
			fix queries that don't have the right objects in the initial database.
			ie, if images are labeled from among a limited set of objects, predict likely additional objects and confirm given context of known objects
	
			using current state for dynamically changing input (ie, video)
		
clean up

	x clean out tools folder
	x clean up the image label loader
	x update analysis scripts
	x wrap in packages
	x see if saved structure for per-agent records can be made smaller. what all is being saved?
	x remove references to local machines

bugs

	x two folders, not in results at all, two data sets, same method, caused error

backlog

	change situate.labl_load to throw errors when label isn't found
	current behavior: returns empty label struct
	better behavior: everything that calls it to use try/catch to generate their own empty label structs when necessary.

	make sure repeated runs are being grouped properly in analysis code
		decide what analysis steps should include run variance (if any)
		at least, a distribution of final support scores, with some error bounds
		generate report of high/low variance images from among tp, fp, tn, fn

	another pass on the readme (new analysis scripts)

	additional readme for extras. there are a few now

	include images in the readme (?.md)

	issue: removing good boxes from workspace and not putting them back into the pool
		i think the way we have it may create a second-attempted-entry advantage with some of the 
		box a is checked in for person 1
		box b is attempted for person 1, rejected
		box a is attempted for person 2, accepted, replaces existing entry for person 1 (higher external support)
		now we've already tried box b for person 1 and moved on, so won't get another shot
	
	issue: if not good stuff in workspace, everything stays bad
		total support should be individual object features, box parameters support, each pair support, full situation support

	issue: thresholds are not set during a run. the pool priming gives us an idea of internal support score distributions, which should be used more



experiments

	situations
		dogwalking
		handhsaking
		pingpong
	data
		positives test set
		large negatives set
		hard-negatives set
	parameters
		uniform
		naive faster rcnn (just top box -> situation score)
		situate as proposed 
			detection-based classifier (not iou est)
			fixed internal-external support values
			local search, no box-adjust
		situate final
			iou estimation classifier
			internal-external support based on AUROC
			box-adjust
			pool priming with rcnn
		situate with faster-rcnn boxes
			(same, but faster-rcnn boxes for priming)
	


old issues

	x update parallel mode
	x include precision recall code, clean up, removing recall @ n stuff
	x save off results more often (every 100 images or so?)
		added function situate.data_generate_split_files_for_long_run
		this makes split files that use the same training set, but split up the testing data in 100 image chunks for saving. no changes needed to experiment_handler
	x clean up the git repo

	x when situate tries to load models from "saved_models," if that directory exists, it crashes.

	x the gt IOU is not correct in the workspace. 
	check for when it's adjusted to make sure the correct boxes are being compared (and that the workspace entry is being properly updated when something goes in and is replaced)

	x add the new RCNN pre-computed box files, make sure that pipeline works properly

	x include workspace / gt reconciliation as a function external to 

	x in the results file, copy the experiment script, situation struct, and situate parameters used in the run

	x when situate tries to load models from "saved_models," if that directory doesn't exist, it crashes.
	make sure that it just creates that directory if it doesn't already exist.

	x the gt IOU is not correct in the workspace. 
		check for when it's adjusted to make sure the correct boxes are being compared (and that the workspace entry is being properly updated when something goes in and is replaced)
		update: added an additional check of correct gt IOU in the main_loop and couldn't reproduce this

	x initialize pool with rcnn data
		add the new RCNN pre-computed box files, make sure that pipeline works properly
		currently crashes because i wanted to check the string comparison
		right now, trying to use the information in the situation structure to automatically pick a directory from the rcnn folder
		just include the directory that contains the relevant rcnn data as an arg?

		"agent_pool_initialization_function" : "@(a,b,c,d) situate.agent.pool_initialize_rcnn(a,b,c,d,10,30);"

		 vs

		"agent_pool_initialization_function" : "@(a,b,c,d) situate.agent.pool_initialize_rcnn(a,b,c,d,10,30,{'rcnn box data/dogwalking/});",

		always have rcnn boxes that match situation objects

		mapping from csv data to situation objects is sorta complicated
		person1 person2, vs person
		person_left person_right


	x training iou regression classifier
		there's a display when running trust rounds that is unclear

	x loading pre-extracted features mat
	 	lots of warnings
		mat has p_struct in it, which has anon functions, which look for their original source in a hardcoded path. not actually used though

		that file should just be the pre-extracted features
		
		a separate function should learn a conditional model and estimate external and total support functions

	x melanie's warnings when starting up matlab

	x continue to update analysis code

	x have an explicit setting for the max number of boxes taken from the rcnn like mechanism

	x issue with negative images in the visualizer

	x check that analysis code works with only negative images

	x include figures that show top results in positives, negatives, false pos, and false neg

	x install matconvnet if not already installed during setup







	
